Bowling Statistics Tracker - Group 5
Team Members: Brandon Loi, Hunter Sullivan, James Smagacz, Aaron Althoff, Aaron Nordhoff
Professor Turkstra
CS307
October 11th, 2017
Sprint 1 Retrospective
1. What went well?
We think that after the implementation of the database, many of the user stories were smoothly developed and inserted into the application. Specifically, this entails commits made after <cda5a55> within the repository. Just two days after (Oct 2), the team began pushing major updates to the repo to implement their user stories.  We were able to successfully implement functionality between the database, front-end, and back-end, including: creating a user, logging into a user, resetting a userâ€™s password, viewing manually entered statistics, looking up other users to compare statistics to (using username or email), and allowing coaches to edit user statistics. 

2. What did not go well?
During the sprint,  the integration between the client and server was probably done later in the process than it should have been. We had intended on using a MySQL database to serve as the backend. This proved to be more difficult than we had initially thought it would be. Both Brandon and Hunter are currently in CS348 and learning about SQL databases and how to connect to them from clients. Unfortunately, their class switched topics during the beginning of the sprint, causing them to struggle with the implementation. For the last week in September, we struggled with various drivers to setup the client server connection (JDBC, Google Cloud Client Library, etc.) before finally switching to Google Firebase. The group found this to be much easier to interact due to its UI and simple API. We also misunderstood an instruction within the planning document. The group believed we were to implement 100 hours worth of user stories for the sprint, but in reality we were to implement enough for 150 hours total for a group of 5 (~10 hours per week per person). This caused us to have to reorganize what our objectives were for the sprint and to figure out which user stories we could implement in the time remaining. Thankfully we were able to pull this off, but it did cause an unintended amount of crunch in the final week of the sprint. When it came time to present, our group encountered a small bug in displaying an error upon unsuccessful signup of an account. We were taken by surprise by this, as this was one of the first user stories we had implemented after setting up the database. If we had implemented automated testing within Android studio, we could have caught what caused this malfunction before the presentation to the TA. Additionally, one of our user stories, the one involving resetting passwords, did not work when we presented it.

3. How should we improve?
For future springs, our group should be prepared with 150 hours worth of user stories to implement from the beginning. This will help prevent the necessity of adding additional user stories at the end of the sprint, which caused an uneven distribution of work and resulted in a heavy time commitment to the project near the end of the sprint. We will also look into running automated tests within Android studio, allowing for easy regression testing and preventing future changes from creating unforeseen bugs in previously written code. Finally, because we were crunched for time at the end of sprint 1, we were focused on functionality of user stories, and less on aesthetics, layout, or appearance of the app.  In future sprints, we will be sure to better plan our work, leaving more time at the end of the sprint to adhere to the appearance of the application, perform user testing on the end result of sprint 2, and polish any minor issues that arise.
